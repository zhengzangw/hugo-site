---
title:
date: 2020-06-09
---

## 基本概念

- 随机变量：$S$
- 信息：消除随机变量不确定性的事物
  - 信息量与传播媒介无关
  - 信息是相对的
  - 信息是客观物理量
- 噪音（非信息）
- 数据 = 噪音 + 信息
- 信源：产生信息的实体
  - 信源符号 $s_i$ 发生概率 $p_i$
- 自信息：$I(s_i)=-\log p_i$
- 信息熵：$H(S)=\sum_{i=1}^np_iI(s_i)$
  - 信源发出符号平均信息量，衡量不确定度
  - 编码的最优策略
  - 二为底：bit
  - e 为底：纳特
- 条件自信息：$I(x_i|y_j)=-\log p(x_i|y_i)$
- 条件熵：$H(X|Y)=E[I(x_i|y_j)]=-\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)\log p(x_i|y_j)$
- 互信息：$I(X|Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)$
  - 接受到一个变量使另一个变量不确定度减少的量
- 联合熵：$H(X,Y)=-\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)\log p(x_i,y_j)$
- 交叉熵：$H(P,Q)=-\sum_{i}p_i\log q_i$
  - $P$ 基于 $Q$ 编码时平均比特数
- 相对熵（KL 散度，信息增益）：$D_{\text{KL}}(P||Q)=-\sum_i p_i\ln\frac{q_i}{p_i}$
  - 使用基于 $Q$ 的分布来编码服从 $P$ 的分布的样本所需的额外的平均比特数
  - $H(p,q) = H(p) + D_{\text{KL}}(P||Q)$
